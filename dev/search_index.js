var documenterSearchIndex = {"docs":
[{"location":"CPPLS/crossvalidation/#Cross-Validation","page":"Cross Validation","title":"Cross Validation","text":"","category":"section"},{"location":"CPPLS/crossvalidation/#StatisticalProjections.calculate_p_value","page":"Cross Validation","title":"StatisticalProjections.calculate_p_value","text":"calculate_p_value(permutation_accuracies::AbstractVector{<:Real},\n                  model_accuracy::Float64)\n\nCompute an empirical p-value from permutation test accuracies. Counts how many permutation  accuracies are less than or numerically equal to the observed model_accuracy, divides by  length(permutation_accuracies) + 1 to include the observed model in the denominator.\n\nArguments\n\npermutation_accuracies: vector of accuracies from label-shuffled runs.\nmodel_accuracy: accuracy achieved by the true model.\n\nExample\n\njulia> calculate_p_value([0.4, 0.5, 0.55, 0.6], 0.58) ≈ 0.6\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/crossvalidation/#StatisticalProjections.nested_cv","page":"Cross Validation","title":"StatisticalProjections.nested_cv","text":"nested_cv(X_predictors::AbstractMatrix{<:Real}, Y_responses::AbstractMatrix{<:Real};\n    gamma::Union{<:Real, <:NTuple{2, <:Real}, <:AbstractVector{<:Union{<:Real,<:NTuple{2, <:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real},Nothing}=nothing,\n    Y_auxiliary::Union{AbstractMatrix{<:Real},Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64),\n    t_squared_norm_tolerance::Real=1e-10,\n    gamma_optimization_tolerance::Real=1e-4,\n    num_outer_folds::Integer=8,\n    num_outer_folds_repeats::Integer=num_outer_folds,\n    num_inner_folds::Integer=7,\n    num_inner_folds_repeats::Integer=num_inner_folds,\n    max_components::Integer=5,\n    weighted_nmc::Bool=true,\n    rng::AbstractRNG=Random.GLOBAL_RNG,\n    verbose::Bool=true)\n\nTop-level nested CV driver for CPPLS. Parameter overview:\n\nX_predictors, Y_responses: feature and one-hot response matrices.\ngamma: either a scalar γ, a (lo, hi) tuple, or a vector of mixed candidates passed to fit_cppls_light. Scalars enforce a single γ for all components, whereas tuples/vectors share candidate ranges from which each component selects its own optimum.\nobservation_weights: optional sample weights; Y_auxiliary: extra response features.\ncenter: toggle mean-centering; tolerances control numerical stability inside inner fits; weighted_nmc chooses between weighted/unweighted misclassification cost.\nnum_outer_folds, num_outer_folds_repeats: number of outer stratified folds and how many to evaluate (≤ num_outer_folds).\nnum_inner_folds, num_inner_folds_repeats: same for inner CV.\nmax_components: maximum latent components considered by the inner loop.\nrng: random generator shared across fold shuffles; verbose: emit progress.\n\nFor every outer fold the data is split into training/test partitions, an inner CV loop selects the optimal component count via optimize_num_latent_variables, the final CPPLSLight is fit on the outer training data with that count, and accuracy = 1 - nmc is computed on the outer test split. Returns a tuple (outer_fold_accuracies, optimal_num_latent_variables).\n\nExample\n\njulia> using Random\n\njulia> X = rand(MersenneTwister(1), 12, 4);\n\njulia> labels = repeat([\"red\", \"blue\", \"green\"], 4);\n\njulia> Y, _ = labels_to_one_hot(labels);\n\njulia> accs, comps = nested_cv(\n           X, Y;\n           gamma=0.5,\n           num_outer_folds=3,\n           num_inner_folds=2,\n           max_components=2,\n           rng=MersenneTwister(2),\n           verbose=false);\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\njulia> accs ≈ [1.1102230246251565e-16, 0.0, 0.0]\ntrue\n\njulia> comps ≈ [1, 1, 1]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/crossvalidation/#StatisticalProjections.nested_cv_permutation","page":"Cross Validation","title":"StatisticalProjections.nested_cv_permutation","text":"nested_cv_permutation(X_predictors::AbstractMatrix{<:Real}, Y_responses::AbstractMatrix{<:Real};\n    gamma::Union{<:Real, <:NTuple{2,<:Real}, <:AbstractVector{<:Union{<:Real,<:NTuple{2,<:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real},Nothing}=nothing,\n    Y_auxiliary::Union{AbstractMatrix{<:Real},Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64),\n    t_squared_norm_tolerance::Real=1e-10,\n    gamma_optimization_tolerance::Real=1e-4,\n    num_outer_folds::Integer=9,\n    num_outer_folds_repeats::Integer=num_outer_folds,\n    num_inner_folds::Integer=8,\n    num_inner_folds_repeats::Integer=num_inner_folds,\n    max_components::Integer=5,\n    weighted_nmc::Bool=true,\n    num_permutations::Integer=999,\n    rng::AbstractRNG=Random.GLOBAL_RNG,\n    verbose::Bool=true)\n\nPermutation-based significance test for the nested CV pipeline. Keywords mirror nested_cv but with explicit defaults suited for permutation tests. Parameter summary:\n\ngamma, observation_weights, Y_auxiliary, center, tolerances, weighted_nmc: forwarded directly into each nested CV call.\nnum_outer_folds, num_outer_folds_repeats, num_inner_folds, num_inner_folds_repeats, max_components: control the inner/outer fold geometry per permutation.\nnum_permutations: number of label shuffles (≥ 1).\nrng: governs shuffling of both labels and folds; verbose: prints progress.\n\nFor each permutation, the rows of Y_responses are randomly shuffled, then nested_cv is executed with the same hyperparameters, and the mean outer-fold accuracy is recorded. Returns a vector of length num_permutations containing those mean accuracies so you can compute empirical p-values against the unpermuted nested-CV accuracy.\n\nExample\n\njulia> using Random\n\njulia> X = rand(MersenneTwister(1), 12, 4);\n\njulia> labels = repeat([\"red\", \"blue\", \"green\"], 4);\n\njulia> Y, classes = labels_to_one_hot(labels);\n\njulia> perms = nested_cv_permutation(X, Y;\n                 gamma=0.5,\n                 num_outer_folds=3,\n                 num_inner_folds=2,\n                 max_components=2,\n                 num_permutations=2,\n                 verbose=false,\n                 rng=MersenneTwister(2));\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n\njulia> perms ≈ [0.3055555555555556, 0.22222222222222224]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/internal/#Internal","page":"Internal","title":"Internal","text":"","category":"section"},{"location":"CPPLS/internal/#StatisticalProjections.nmc","page":"Internal","title":"StatisticalProjections.nmc","text":"StatisticalProjections.nmc(Y_true_one_hot::AbstractMatrix{<:Integer}, \n    Y_pred_one_hot::AbstractMatrix{<:Integer}, weighted::Bool)\n\nCompute the normalized misclassification cost between true and predicted one-hot label  matrices. If weighted is false, the function returns the plain misclassification rate  (mean of entry-wise inequality). When true, class weights inversely proportional to  their prevalence are applied, so rare classes contribute equally.\n\nArguments\n\nY_true_one_hot: (n_samples × n_classes) ground truth one-hot labels.\nY_pred_one_hot: predicted one-hot labels of the same shape.\nweighted: toggle class-balanced weighting.\n\nReturns a Float64 between 0 and 1.\n\nExample\n\njulia> Y_true = [1 0 0; 0 1 0; 0 1 0];\n\njulia> Y_pred = [1 0 0; 0 0 1; 0 1 0];\n\njulia> StatisticalProjections.nmc(Y_true, Y_pred, false) ≈ 0.2222222222222222\ntrue\n\njulia> StatisticalProjections.nmc(Y_true, Y_pred, true) ≈ 0.25\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/internal/#StatisticalProjections.optimize_num_latent_variables","page":"Internal","title":"StatisticalProjections.optimize_num_latent_variables","text":"StatisticalProjections.optimize_num_latent_variables(\n    X_train_full::AbstractMatrix{<:Real},\n    Y_train_full::AbstractMatrix{<:Integer},\n    max_components::Integer,\n    num_inner_folds::Integer,\n    num_inner_folds_repeats::Integer,\n    gamma::Union{<:Real, <:NTuple{2,<:Real}, <:AbstractVector{<:Union{<:Real, <:NTuple{2, <:Real}}}},\n    observation_weights::Union{AbstractVector{<:Real},Nothing},\n    Y_auxiliary::Union{AbstractMatrix{<:Real},Nothing},\n    center::Bool,\n    X_tolerance::Real,\n    X_loading_weight_tolerance::Real,\n    t_squared_norm_tolerance::Real,\n    gamma_optimization_tolerance::Real,\n    weighted_nmc::Bool,\n    rng::AbstractRNG,\n    verbose::Bool)\n\nRepeated inner cross-validation used inside nested_cv to pick the component count. Argument summary:\n\nX_train_full, Y_train_full: numeric training matrices (observations × features/targets).\nmax_components: Int upper bound on components to evaluate (≥ 1).\nnum_inner_folds, num_inner_folds_repeats: integers controlling stratified folds drawn via random_batch_indices.\ngamma: either a scalar γ, a (lo, hi) tuple of Reals, or a vector mixing both; forwarded to fit_cppls_light. Scalars keep γ fixed for every component, while tuples/vectors let each component pick the best γ from the shared candidate set.\nobservation_weights: optional weight vector matching the training rows.\nY_auxiliary: optional auxiliary response matrix aligned with Y_train_full.\ncenter: Bool toggling mean-centering in the inner fits.\nX_tolerance, X_loading_weight_tolerance, t_squared_norm_tolerance, gamma_optimization_tolerance: Real tolerances passed to the fitter.\nweighted_nmc: choose class-weighted misclassification cost (true by default).\nrng: random-number generator used for shuffling.\nverbose: when true, prints per-fold diagnostics.\n\nExample\n\njulia> using Random\n\njulia> X = rand(MersenneTwister(1), 12, 4);\n\njulia> labels = repeat([\"red\", \"blue\", \"green\"], 4);\n\njulia> Y, _ = labels_to_one_hot(labels);\n\njulia> k = StatisticalProjections.optimize_num_latent_variables(\n             X, Y,\n             2,\n             3, 3,\n             0.5,\n             nothing, nothing,\n             true,\n             1e-12, eps(Float64), 1e-10,\n             1e-4,\n             true,\n             MersenneTwister(2),\n             false);\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\njulia> k\n1\n\nFor every inner repeat the routine fits a CPPLSLight with max_components, scores validation folds for each partial component count using predict + predictonehot, evaluates nmc, records the argmin, and finally returns the median winning component number (rounded down) across repeats.\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/internal/#StatisticalProjections.random_batch_indices","page":"Internal","title":"StatisticalProjections.random_batch_indices","text":"StatisticalProjections.random_batch_indices(strata::AbstractVector{<:Integer},\n    num_batches::Integer, rng::AbstractRNG=Random.GLOBAL_RNG)\n\nConstruct stratified folds. For each unique entry in strata the corresponding sample indices are shuffled with rng and then dealt round-robin into num_batches disjoint vectors. This keeps class proportions stable across folds. Throws if num_batches is less than 1 or larger than the number of samples. Returns a vector-of-vectors of 1-based indices, each representing one fold.\n\nExample\n\njulia> using Random; rng = MersenneTwister(1);\n\njulia> folds = StatisticalProjections.random_batch_indices([1, 1, 2, 2, 2, 1], 3, rng)\n3-element Vector{Vector{Int64}}:\n [5, 6]\n [4, 1]\n [3, 2]\n\n\n\n\n\n","category":"function"},{"location":"Utils/internal/#Internal","page":"Internal","title":"Internal","text":"","category":"section"},{"location":"Utils/internal/#StatisticalProjections.robustcor","page":"Internal","title":"StatisticalProjections.robustcor","text":"StatisticalProjections.robustcor(x::AbstractVector, y::AbstractVector)\n\nRobust correlation helper used inside projection diagnostics. Returns the Pearson  correlation between x and y, falling back to 0.0 when either input is constant or  when the computed value is not finite (e.g. NaN or Inf).\n\nExamples\n\njulia> StatisticalProjections.robustcor([1, 2, 3], [3, 2, 1])\n-1.0\n\njulia> StatisticalProjections.robustcor([1, 1, 1], [2, 3, 4])\n0.0\n\n\n\n\n\n","category":"function"},{"location":"Utils/matrix/#Matrix","page":"Matrix","title":"Matrix","text":"","category":"section"},{"location":"Utils/matrix/#StatisticalProjections.find_invariant_and_variant_columns","page":"Matrix","title":"StatisticalProjections.find_invariant_and_variant_columns","text":"find_invariant_and_variant_columns(M::AbstractMatrix)\n\nScan each column of M and split them into two index vectors: columns whose entries are all  identical (invariant_columns) and columns that contain any variation (variant_columns).  Useful for removing zero-variance predictors before fitting.\n\nExample\n\njulia> invariant, variant = find_invariant_and_variant_columns([1 2 2; 1 3 4])\n([1], [2, 3])\n\n\n\n\n\n","category":"function"},{"location":"Utils/encoding/#Encoding","page":"Encoding","title":"Encoding","text":"","category":"section"},{"location":"Utils/encoding/#StatisticalProjections.labels_to_one_hot","page":"Encoding","title":"StatisticalProjections.labels_to_one_hot","text":"labels_to_one_hot(label_indices::AbstractVector{<:Integer}, n_labels::Integer)\n\nConvert integer label indices (1-based) to a dense one-hot encoded matrix with n_labels  columns. This variant assumes the set of classes is already known and returns only the  encoded array.\n\nExample\n\njulia> labels_to_one_hot([1, 3, 2], 3)\n3×3 Matrix{Int64}:\n 1  0  0\n 0  0  1\n 0  1  0\n\n\n\n\n\nlabels_to_one_hot(labels::AbstractVector)\n\nEncode arbitrary labels (e.g., strings, integers, symbols) into a one-hot matrix,  automatically determining the unique label ordering. Returns a tuple of the encoded matrix  and the ordered list of labels so callers can map predictions back to the original domain.\n\nExample\n\njulia> matrix, classes = labels_to_one_hot([\"cat\", \"dog\", \"cat\"])\n([1 0; 0 1; 1 0], [\"cat\", \"dog\"])\n\n\n\n\n\n","category":"function"},{"location":"Utils/encoding/#StatisticalProjections.one_hot_to_labels","page":"Encoding","title":"StatisticalProjections.one_hot_to_labels","text":"one_hot_to_labels(one_hot_matrix::AbstractMatrix{<:Integer})\n\nDecode one-hot rows back into label indices by selecting the column of the maximum entry for each row. Works with any integer-valued matrix containing a single positive entry per row.\n\nExample\n\njulia> one_hot_to_labels([1 0 0; 0 1 0; 0 0 1])\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"CPPLS/types/#StatisticalProjections.AbstractCPPLS","page":"Types","title":"StatisticalProjections.AbstractCPPLS","text":"AbstractCPPLS\n\nCommon supertype for Canonical Powered Partial Least Squares models. Any subtype must expose at least the following fields so shared functions can operate generically:\n\nregression_coefficients::Array{<:Real, 3}\nX_means::Matrix{<:Real}\nY_means::Matrix{<:Real}\n\nAdditionally, subtypes are expected to work with the exported generic helpers predict, predictonehot, and project.\n\n\n\n\n\n","category":"type"},{"location":"CPPLS/types/#StatisticalProjections.CPPLS","page":"Types","title":"StatisticalProjections.CPPLS","text":"CPPLS{T1, T2}\n\nFull CPPLS model storing all intermediate quantities required for diagnostics and visualisation. T1 is the floating-point element type used for continuous arrays, T2 is the integer type used for boolean-like masks.\n\nFields\n\nregression_coefficients::Array{T1, 3} — cumulative regression matrices for the  first k = 1 … n_components latent variables.\nX_scores::Matrix{T1} — predictor scores per component.\nX_loadings::Matrix{T1} — predictor loadings per component.\nX_loading_weights::Matrix{T1} — predictor weight vectors per component.\nY_scores::Matrix{T1} — response scores derived from the fitted model.\nY_loadings::Matrix{T1} — response loadings per component.\nprojection::Matrix{T1} — mapping from centred predictors to component scores.\nX_means::Matrix{T1} — row vector of predictor means used for centering.\nY_means::Matrix{T1} — row vector of response means used for centering.\nfitted_values::Array{T1,3} — fitted responses for the first k components.\nresiduals::Array{T1,3} — residual cubes matching fitted_values.\nX_variance::Vector{T1} — variance explained in X per component.\nX_total_variance::T1 — total variance present in the centred predictors.\ngammas::Vector{T1} — power-parameter selections per component.\ncanonical_correlations::Vector{T1} — squared canonical correlations per component.\nsmall_norm_indices::Matrix{T2} — boolean mask of columns deflated to zero.\ncanonical_coefficients::Matrix{T1} — canonical coefficient matrix from CCA.\n\n\n\n\n\n","category":"type"},{"location":"CPPLS/types/#StatisticalProjections.CPPLSLight","page":"Types","title":"StatisticalProjections.CPPLSLight","text":"CPPLSLight{T}\n\nMemory-lean CPPLS variant retaining only the pieces needed for prediction. T is the floating-point element type shared by all stored matrices.\n\nFields\n\nregression_coefficients::Array{T, 3} — stacked regression matrices.\nX_means::Matrix{T} — predictor means copied from the training data.\nY_means::Matrix{T} — response means copied from the training data.\n\n\n\n\n\n","category":"type"},{"location":"Utils/statistics/#Statistics","page":"Statistics","title":"Statistics","text":"","category":"section"},{"location":"Utils/statistics/#StatisticalProjections.fisherztrack","page":"Statistics","title":"StatisticalProjections.fisherztrack","text":"fisherztrack(X::AbstractArray{<:Real, 3}, scores::AbstractVector; weights=:mean)\n\nInterpret X as a three-dimensional array (a “tensor”) of shape n × axis₁ × axis₂, where n matches the length of scores. For every combination of axis₁ and axis₂,  the function extracts the length-n slice (a single “track”) and correlates it with  scores. Those correlations are Fisher z-transformed to stabilize variance, optionally  weighted by the slice means (when weights = :mean), averaged for each axis₁, and  finally inverse-transformed. The result is one smoothed correlation per axis₁,  summarizing all its axis₂ slices.\n\nArguments\n\nX: 3-D tensor whose first axis matches the observation axis of scores.\nscores: response to correlate with every slice in X.\nweights: choose :mean to weight by the slice means or :none for equal weights.\n\nReturns a vector of correlations with length size(X, 2).\n\nExample\n\njulia> X = reshape(Float64[1, 2, 3, 4, 2, 3, 4, 5, 3, 5, 7, 9], 4, 3, 1);\n\njulia> scores = [1.0, 2.0, 3.0, 4.0];\n\njulia> fisherztrack(X, scores) ≈ [1.0, 1.0, 1.0]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"Utils/statistics/#StatisticalProjections.separationaxis","page":"Statistics","title":"StatisticalProjections.separationaxis","text":"separationaxis(Xscores::AbstractMatrix, Y::AbstractMatrix; method::Symbol=:centroid, \npositive_class::Integer=1)\n\nGiven Xscores (rows = samples, columns = latent components) and a binary one-hot label  matrix Y with two columns, this helper finds the line in score space that best separates  the classes. It returns (direction, scores) where direction is a unit vector and  scores = Xscores * direction are the signed projections, flipped if necessary so that the  positive_class has larger values.\n\nWhen Xscores has multiple columns, choose method = :centroid to use the difference of  class means or method = :lda to use Fisher’s linear discriminant (pooled covariance). If  Xscores has only one column, the function just selects the orientation that makes the  positive_class larger on average.\n\nExamples\n\njulia> X = [1.0 0.0; 2.0 1.0; 0.0 1.0; 1.0 2.0];\n\njulia> Y = [1 0; 1 0; 0 1; 0 1];\n\njulia> direction, scores = separationaxis(X, Y; method=:centroid);\njulia> direction ≈ [0.7071067811865474, -0.7071067811865474]\ntrue\n\njulia> scores ≈ [0.7071067811865474, 0.7071067811865474, -0.7071067811865474, -0.7071067811865474]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"#StatisticalProjections.jl","page":"Home","title":"StatisticalProjections.jl","text":"StatisticalProjections provides chemometric projection methods in Julia, currently centered on CPPLS-DA for supervised classification tasks. The goal is to enable reproducible preprocessing, fitting, and validation pipelines, so common chemometric analyses stay transparent and auditable.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"The package is not registered, so install it directly from GitHub:\n\njulia> ]\npkg> add https://github.com/oniehuis/StatisticalProjections.jl\n\nAfter the installation finishes you can load it in the Julia REPL with:\n\njulia> using StatisticalProjections","category":"section"},{"location":"#Current-capabilities","page":"Home","title":"Current capabilities","text":"A pure-Julia implementation of Canonical Powered Partial Least Squares Discriminant Analysis (CPPLS-DA; Indahl et al. 2019, Liland & Indahl 2009) that handles collinear  predictors and exports interpretable loadings and scores.\nCross-validation utilities (nested_cv, nested_cv_permutation) for selecting the number of latent components and estimating classification performance or permutation baselines (Smit et al. 2007; currently only validated for discriminant/classification models).\nPermutation-based significance testing via calculate_p_value to quantify whether observed accuracies exceed what would be expected by chance.\nOptional preprocessing utilities so that scaling, centering, or other chemometric transformations can be folded into the modeling workflow.","category":"section"},{"location":"#Quick-taste","page":"Home","title":"Quick taste","text":"using StatisticalProjections\nusing Random\nusing Statistics\n\nrng = MersenneTwister(1)\nX = randn(rng, 60, 30)                                     # predictors (e.g., spectra)\nlabels = repeat([\"classA\", \"classB\"], inner=30)\nY, _ = labels_to_one_hot(labels)\n\naccuracies, components = nested_cv(\n    X, Y;\n    max_components=2,\n    num_outer_folds=3,\n    num_inner_folds=2,\n    rng=rng,\n    verbose=false)\n\nbest_components = floor(Int, median(components))           # consensus components across folds\nmodel = fit_cppls_light(X, Y, best_components; gamma=0.5)\nŷ = predictonehot(model, predict(model, X))                # fitted class indicators\n\npermutation_scores = nested_cv_permutation(\n    X, Y;\n    max_components=2,\n    num_outer_folds=3,\n    num_inner_folds=2,\n    num_permutations=25,\n    rng=rng,\n    verbose=false)\n\np_value = calculate_p_value(permutation_scores, mean(accuracies))\n\nThe calculated p_value reports the empirical probability of obtaining mean accuracies this high when class labels are randomly permuted, so smaller values suggest structure unlikely to arise from chance alone.","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"Learn how to fit models (options, preprocessing, γ-search) in CPPLS/fit.md and how to generate projections or class predictions in CPPLS/predict.md.\nDive into the cross-validation and permutation tooling described in CPPLS/crossvalidation.md.\nInspect the underlying data structures (CPPLS, CPPLSLight, preprocessing helpers) once you need finer control in CPPLS/types.md and CPPLS/internal.md.","category":"section"},{"location":"#Disclaimer","page":"Home","title":"Disclaimer","text":"StatisticalProjections is research software provided “as is.” You remain responsible for validating every discriminant analysis and any downstream decision or deployment based on these models; the authors cannot be held liable if the algorithms produce misleading or incorrect results.","category":"section"},{"location":"#References","page":"Home","title":"References","text":"Indahl UG, Liland KH, Naes T (2009) Canonical partial least squares — a unified PLS  approach to classification and regression problems. Journal of Chemometrics 23: 495-504.  https://doi.org/10.1002/cem.1243.\nLiland KH, Indahl UG (2009): Powered partial least squares discriminant analysis.  Journal of Chemometrics 23: 7-18. https://doi.org/10.1002/cem.1186.\nSmit S, van Breemen MJ, Hoefsloot HCJ, Smilde AK, Aerts JMFG, de Koster CG (2007):  Assessing the statistical validity of proteomics based biomarkers. Analytica Chimica  Acta 592: 210-217. https://doi.org/10.1016/j.aca.2007.04.043.","category":"section"},{"location":"CPPLS/fit/#Fit","page":"Fit","title":"Fit","text":"","category":"section"},{"location":"CPPLS/fit/#StatisticalProjections.fit_cppls","page":"Fit","title":"StatisticalProjections.fit_cppls","text":"fit_cppls(\n    X::AbstractMatrix{<:Real},\n    Y::AbstractMatrix{<:Real},\n    n_components::Integer;\n    gamma::Union{<:Real, <:NTuple{2, <:Real}, <:AbstractVector{<:Union{<:Real, <:NTuple{2, <:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real}, Nothing}=nothing,\n    Y_auxiliary::Union{AbstractMatrix{<:Real}, Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64), \n    gamma_optimization_tolerance::Real=1e-4,\n    t_squared_norm_tolerance::Real=1e-10)\n\nFit a Canonical Powered Partial Least Squares (CPPLS) model.\n\nArguments\n\nX: A matrix of predictor variables (observations × features). NAs and Infs are not  allowed.\nY: A matrix of response variables (observations × targets). NAs and Infs are not  allowed.\n\nOptional Positional Argument\n\nn_components: The number of components to extract in the CPPLS model. Defaults to 2.\n\nOptional Keyword Arguments\n\ngamma: Either (i) a fixed power parameter (γ), (ii) a (lo, hi) tuple describing the bounds for per-component optimization, or (iii) a vector mixing both forms. Defaults to 0.5, i.e. no optimization.\nobservation_weights: A vector of individual weights for the observations (e.g.,  experimental data or samples). Defaults to nothing.\nY_auxiliary: A matrix of auxiliary response variables containing additional information  about the observations. Defaults to nothing.\ncenter: Whether to mean-center the X and Y matrices. Defaults to true.\nX_tolerance: Tolerance for small norms in X. Columns of X with norms below this  threshold are set to zero during deflation. Defaults to 1e-12.\nX_loading_weight_tolerance: Tolerance for small weights. Elements of the weight vector  below this threshold are set to zero. Defaults to eps(Float64).\ngamma_optimization_tolerance: Tolerance for the optimization process when determining   the power parameter (γ). Defaults to 1e-4.\nt_squared_norm_tolerance: Small positive value added to near-zero score norms to keep downstream divisions stable. Defaults to 1e-10.\n\nReturns\n\nA CPPLS object containing the following fields:\n\nregression_coefficients: A 3D array of regression coefficients for 1, ...,  n_components.\nX_scores: A matrix of scores (latent variables) for the predictor matrix X.\nX_loadings: A matrix of loadings for the predictor matrix X.\nX_loading_weights: A matrix of loading weights for the predictor matrix X.\nY_scores: A matrix of scores (latent variables) for the response matrix Y.\nY_loadings: A matrix of loadings for the response matrix Y.\nprojection: The projection matrix used to convert X to scores.\nX_means: A vector of means of the X variables (used for centering).\nY_means: A vector of means of the Y variables (used for centering).\nfitted_values: An array of fitted values for the response matrix Y.\nresiduals: An array of residuals for the response matrix Y.\nX_variance: A vector containing the amount of variance in X explained by each   component.\nX_total_variance: The total variance in X.\ngammas: The power parameter (γ) values obtained during power optimization.\ncanonical_correlations: Canonical correlation values for each component.\nsmall_norm_indices: Indices of explanatory variables with norms close to or equal to   zero.\ncanonical_coefficients: A matrix containing the canonical coefficients (a) from  canonical correlation analysis (cor(Za, Yb)).\n\nNotes\n\nThe CPPLS model is an extension of Partial Least Squares (PLS) that incorporates  canonical correlation analysis (CCA) and power parameter optimization to maximize the  correlation between linear combinations of X and Y.\nThe power parameter (γ) controls the balance between variance maximization and  correlation maximization. It is optimized within the specified bounds (gamma_bounds).\nIf Y_auxiliary is provided, it is concatenated with Y to form a combined response  matrix (Y_combined), which is used during the fitting process.\n\nExample\n\njulia> X = Float64[1 0 2\n                   0 1 2\n                   1 1 1\n                   2 3 0\n                   3 2 1];\n\njulia> labels = [\"red\", \"blue\", \"red\", \"blue\", \"red\"];\n\njulia> Y, classes = labels_to_one_hot(labels);\n\njulia> model = fit_cppls(X, Y, 2; gamma=(0.7, 1.0));\n\njulia> model.X_means ≈ Matrix([1.4 1.4 1.2])\n\njulia> model.gammas ≈ [0.700185836799654, 0.9366214237592033]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/fit/#StatisticalProjections.fit_cppls_light","page":"Fit","title":"StatisticalProjections.fit_cppls_light","text":"fit_cppls_light(\n    X::AbstractMatrix{<:Real},\n    Y::AbstractMatrix{<:Real},\n    n_components::Integer;\n    gamma::Union{<:Real, <:NTuple{2, <:Real}, <:AbstractVector{<:Union{<:Real, <:NTuple{2, <:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real}, Nothing}=nothing,\n    Y_auxiliary::Union{AbstractMatrix{<:Real}, Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64),\n    gamma_optimization_tolerance::Real=1e-4,\n    t_squared_norm_tolerance::Real=1e-10)\n\nFit a CPPLS model but retain only the parts needed for prediction (CPPLSLight).\n\nArguments mirror fit_cppls, including support for scalar γ, (lo, hi) bounds, or vectors that mix scalars and tuples as candidate sets. The returned CPPLSLight stores only the stacked regression coefficients plus the X/Y centering means.\n\nNotes\n\nUse this when you only need predictions, not the intermediate diagnostics.\nThe same preprocessing, weighting, and tolerance settings apply as in fit_cppls.\n\nExample\n\njulia> X = Float64[1 0 2\n                   0 1 2\n                   1 1 1\n                   2 3 0\n                   3 2 1];\n\njulia> labels = [\"red\", \"blue\", \"red\", \"blue\", \"red\"];\n\njulia> Y, classes = labels_to_one_hot(labels);\n\njulia> model = fit_cppls_light(X, Y, 2; gamma=(0.7, 1.0));\n\njulia> model.X_means ≈ Matrix([1.4 1.4 1.2])\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#Predict","page":"Predict","title":"Predict","text":"","category":"section"},{"location":"CPPLS/predict/#StatisticalProjections.predict","page":"Predict","title":"StatisticalProjections.predict","text":"predict(cppls::CPPLS, X::AbstractMatrix{<:Real},\n    n_components::Integer=size(cppls.regression_coefficients, 3)) -> Array{Float64, 3}\n\nGenerate predictions from a fitted CPPLS model for a given input matrix X.\n\nArguments\n\ncppls: A fitted CPPLS model, containing regression coefficients and mean values of  predictors and responses.\nX: A matrix of predictor variables of size (n_samples_X, n_features).\nn_components (optional): Number of CPPLS components to use for prediction. Defaults to  the full number trained in the model. Must not exceed the number available.\n\nReturns\n\nA 3-dimensional array of shape (n_samples_X, n_targets_Y, n_components):\nn_samples_X: Number of input samples (rows of X)\nn_targets_Y: Number of target variables in the CPPLS model\nn_components: Number of components used for prediction Each [:,:,i] slice corresponds to predictions using the first i components.\n\nExample\n\njulia> coeffs = reshape(Float64[0.5, 1.0], 2, 1, 1);  # two predictors, one target\n\njulia> X_mean = zeros(1, 2); Y_mean = reshape([0.0], 1, 1);\n\njulia> model = CPPLSLight(coeffs, X_mean, Y_mean);\n\njulia> Xnew = [1.0 2.0; 3.0 4.0];\n\njulia> predict(model, Xnew) ≈ [2.5; 5.5]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#StatisticalProjections.predictonehot","page":"Predict","title":"StatisticalProjections.predictonehot","text":"predictonehot(cppls::AbstractCPPLS, predictions::AbstractArray{<:Real, 3}) -> Matrix{Int}\n\nConvert a 3D array of predictions from a CPPLS model into a one-hot encoded 2D matrix,  assigning each sample to the class with the highest summed prediction across components,  after adjusting for overcounted means.\n\nArguments\n\ncppls: A fitted CPPLS model object containing the mean response vector (Y_means).\npredictions: A 3D array of predictions with dimensions (n_samples_X, n_targets_Y,  n_components).  Typically, this represents predicted values for multiple samples, targets, and components.\n\nReturns\n\nA 2D matrix of size (n_samples_X, n_targets_Y) where each row is a one-hot encoded  vector indicating the target class with the highest summed prediction across components.\n\nDetails\n\nSums predictions across all components for each sample and class.\nAdjusts the summed predictions by subtracting (n_components - 1) times the mean  response, to correct for repeated addition of the mean in each component.\nFor each sample, finds the class index with the highest adjusted prediction.\nConverts the predicted class indices to a one-hot encoded matrix.\n\nExample\n\njulia> coeffs = reshape(Float64[1, -1, 0.5, -0.5], 2, 2, 1);  # two predictors, two classes\n\njulia> X_mean = zeros(1, 2); Y_mean = reshape([0.0 0.0], 1, 2);\n\njulia> model = CPPLSLight(coeffs, X_mean, Y_mean);\n\njulia> Xnew = [2.0 1.0; 0.5 3.0];\n\njulia> raw = predict(model, Xnew);  # size 2×2×1\n\njulia> raw ≈ [1.0 0.5; -2.5 -1.25]\ntrue\n\njulia> predictonehot(model, raw) ≈ [1 0; 0 1]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#StatisticalProjections.project","page":"Predict","title":"StatisticalProjections.project","text":"project(cppls::AbstractCPPLS, X::AbstractMatrix{<:Real}) -> AbstractMatrix\n\nCompute latent component scores by projecting new predictors X with a fitted CPPLS model.\n\nArguments\n\ncppls: Any CPPLS model (e.g., CPPLS or CPPLSLight) providing X_means and projection.\nX: Predictor matrix shaped like the training data (n_samples × n_features).\n\nReturns\n\nMatrix of size (n_samples, n_components) containing the component scores.\n\nDetails\n\nCenters X by subtracting cppls.X_means, then multiplies by the projection matrix.\n\nExample\n\njulia> struct DemoCPPLS <: StatisticalProjections.AbstractCPPLS\n           projection::Matrix{Float64}\n           X_means::Matrix{Float64}\n       end\n\njulia> proj = reshape([1.0, 0.5], 2, 1)\n2×1 Matrix{Float64}:\n 1.0\n 0.5\n\njulia> demo = DemoCPPLS(proj, reshape([0.5, 0.5], 1, :));\n\njulia> project(demo, [1.0 2.0; 3.0 4.0]) ≈ [1.25; 4.25]\ntrue\n\nIn practice, demo would be the CPPLS object returned by fit_cppls, which already contains the appropriate projection matrix and predictor means.\n\n\n\n\n\n","category":"function"}]
}
