var documenterSearchIndex = {"docs":
[{"location":"Utils/matrix/#Matrix","page":"Matrix","title":"Matrix","text":"","category":"section"},{"location":"Utils/matrix/#StatisticalProjections.find_invariant_and_variant_columns","page":"Matrix","title":"StatisticalProjections.find_invariant_and_variant_columns","text":"find_invariant_and_variant_columns(M::AbstractMatrix)\n\nScan each column of M and split them into two index vectors: columns whose entries are all  identical (invariant_columns) and columns that contain any variation (variant_columns).  Useful for removing zero-variance predictors before fitting.\n\nExample\n\njulia> invariant, variant = find_invariant_and_variant_columns([1 2 2; 1 3 4])\n([1], [2, 3])\n\n\n\n\n\n","category":"function"},{"location":"Utils/encoding/#Encoding","page":"Encoding","title":"Encoding","text":"","category":"section"},{"location":"Utils/encoding/#StatisticalProjections.labels_to_one_hot","page":"Encoding","title":"StatisticalProjections.labels_to_one_hot","text":"labels_to_one_hot(label_indices::AbstractVector{<:Integer}, n_labels::Integer)\n\nConvert integer label indices (1-based) to a dense one-hot encoded matrix with n_labels  columns. This variant assumes the set of classes is already known and returns only the  encoded array.\n\nExample\n\njulia> labels_to_one_hot([1, 3, 2], 3)\n3×3 Matrix{Int64}:\n 1  0  0\n 0  0  1\n 0  1  0\n\n\n\n\n\nlabels_to_one_hot(labels::AbstractVector)\n\nEncode arbitrary labels (e.g., strings, integers, symbols) into a one-hot matrix,  automatically determining the unique label ordering. Returns a tuple of the encoded matrix  and the ordered list of labels so callers can map predictions back to the original domain.\n\nExample\n\njulia> matrix, classes = labels_to_one_hot([\"cat\", \"dog\", \"cat\"])\n([1 0; 0 1; 1 0], [\"cat\", \"dog\"])\n\n\n\n\n\n","category":"function"},{"location":"Utils/encoding/#StatisticalProjections.one_hot_to_labels","page":"Encoding","title":"StatisticalProjections.one_hot_to_labels","text":"one_hot_to_labels(one_hot_matrix::AbstractMatrix{<:Integer})\n\nDecode one-hot rows back into label indices by selecting the column of the maximum entry for each row. Works with any integer-valued matrix containing a single positive entry per row.\n\nExample\n\njulia> one_hot_to_labels([1 0 0; 0 1 0; 0 0 1])\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"CPPLS/types/#StatisticalProjections.AbstractCPPLS","page":"Types","title":"StatisticalProjections.AbstractCPPLS","text":"AbstractCPPLS\n\nCommon supertype for Canonical Powered Partial Least Squares models. Any subtype must expose at least the following fields so shared functions can operate generically:\n\nregression_coefficients::Array{<:Real, 3}\nX_means::Matrix{<:Real}\nY_means::Matrix{<:Real}\n\nAdditionally, subtypes are expected to work with the exported generic helpers predict, predictonehot, and project.\n\n\n\n\n\n","category":"type"},{"location":"CPPLS/types/#StatisticalProjections.CPPLS","page":"Types","title":"StatisticalProjections.CPPLS","text":"CPPLS{T1, T2}\n\nFull CPPLS model storing all intermediate quantities required for diagnostics and visualisation. T1 is the floating-point element type used for continuous arrays, T2 is the integer type used for boolean-like masks.\n\nFields\n\nregression_coefficients::Array{T1, 3} — cumulative regression matrices for the  first k = 1 … n_components latent variables.\nX_scores::Matrix{T1} — predictor scores per component.\nX_loadings::Matrix{T1} — predictor loadings per component.\nX_loading_weights::Matrix{T1} — predictor weight vectors per component.\nY_scores::Matrix{T1} — response scores derived from the fitted model.\nY_loadings::Matrix{T1} — response loadings per component.\nprojection::Matrix{T1} — mapping from centred predictors to component scores.\nX_means::Matrix{T1} — row vector of predictor means used for centering.\nY_means::Matrix{T1} — row vector of response means used for centering.\nfitted_values::Array{T1,3} — fitted responses for the first k components.\nresiduals::Array{T1,3} — residual cubes matching fitted_values.\nX_variance::Vector{T1} — variance explained in X per component.\nX_total_variance::T1 — total variance present in the centred predictors.\ngammas::Vector{T1} — power-parameter selections per component.\ncanonical_correlations::Vector{T1} — squared canonical correlations per component.\nsmall_norm_indices::Matrix{T2} — boolean mask of columns deflated to zero.\ncanonical_coefficients::Matrix{T1} — canonical coefficient matrix from CCA.\nsample_labels::AbstractVector — optional labels describing each observation.\npredictor_labels::AbstractVector — optional labels for predictor columns.\nresponse_labels::AbstractVector — labels for regression responses or DA classes.\nanalysis_mode::Symbol — either :regression or :discriminant.\nda_categories — original categorical responses for DA models (nothing otherwise).\n\n\n\n\n\n","category":"type"},{"location":"CPPLS/types/#StatisticalProjections.CPPLSLight","page":"Types","title":"StatisticalProjections.CPPLSLight","text":"CPPLSLight{T}\n\nMemory-lean CPPLS variant retaining only the pieces needed for prediction. T is the floating-point element type shared by all stored matrices.\n\nFields\n\nregression_coefficients::Array{T, 3} — stacked regression matrices.\nX_means::Matrix{T} — predictor means copied from the training data.\nY_means::Matrix{T} — response means copied from the training data.\nanalysis_mode::Symbol — either :regression or :discriminant.\n\n\n\n\n\n","category":"type"},{"location":"CPPLS/fit/#Fit","page":"Fit","title":"Fit","text":"","category":"section"},{"location":"CPPLS/fit/#StatisticalProjections.fit_cppls","page":"Fit","title":"StatisticalProjections.fit_cppls","text":"fit_cppls(\n    X::AbstractMatrix{<:Real},\n    Y::AbstractMatrix{<:Real},\n    n_components::Integer;\n    gamma::Union{<:Real, <:NTuple{2, <:Real}, <:AbstractVector{<:Union{<:Real, <:NTuple{2, <:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real}, Nothing}=nothing,\n    Y_auxiliary::Union{LinearAlgebra.AbstractVecOrMat, Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64), \n    gamma_optimization_tolerance::Real=1e-4,\n    t_squared_norm_tolerance::Real=1e-10,\n    sample_labels::AbstractVector=String[],\n    predictor_labels::AbstractVector=String[],\n    response_labels::AbstractVector=String[])\n\nFit a Canonical Powered Partial Least Squares (CPPLS) model.\n\nArguments\n\nX: A matrix of predictor variables (observations × features). NAs and Infs are not  allowed.\nY: A matrix of response variables (observations × targets). NAs and Infs are not  allowed.\n\nOptional Positional Argument\n\nn_components: The number of components to extract in the CPPLS model. Defaults to 2.\n\nOptional Keyword Arguments\n\ngamma: Either (i) a fixed power parameter (γ), (ii) a (lo, hi) tuple describing the bounds for per-component optimization, or (iii) a vector mixing both forms. Defaults to 0.5, i.e. no optimization.\nobservation_weights: A vector of individual weights for the observations (e.g.,  experimental data or samples). Defaults to nothing.\nY_auxiliary: A matrix (or vector) of auxiliary response variables containing additional information  about the observations. Defaults to nothing.\ncenter: Whether to mean-center the X and Y matrices. Defaults to true.\nX_tolerance: Tolerance for small norms in X. Columns of X with norms below this  threshold are set to zero during deflation. Defaults to 1e-12.\nX_loading_weight_tolerance: Tolerance for small weights. Elements of the weight vector  below this threshold are set to zero. Defaults to eps(Float64).\ngamma_optimization_tolerance: Tolerance for the optimization process when determining   the power parameter (γ). Defaults to 1e-4.\nt_squared_norm_tolerance: Small positive value added to near-zero score norms to keep downstream divisions stable. Defaults to 1e-10.\nsample_labels: Optional labels describing each observation. Defaults to String[].\npredictor_labels: Optional labels for the predictor columns (in order). Defaults to  String[].\nresponse_labels: Optional labels for the response variables / classes (in order). Defaults to String[] for regressions. When passing categorical responses (see below), class labels are inferred automatically.\nanalysis_mode: Internal flag distinguishing regression from discriminant analysis. Advanced callers can override this, but public wrappers set it automatically.\nda_categories: Original categorical responses for discriminant analysis. This is set by the label-based wrapper and must remain nothing for regression problems.\n\nReturns\n\nA CPPLS object containing the following fields:\n\nregression_coefficients: A 3D array of regression coefficients for 1, ...,  n_components.\nX_scores: A matrix of scores (latent variables) for the predictor matrix X.\nX_loadings: A matrix of loadings for the predictor matrix X.\nX_loading_weights: A matrix of loading weights for the predictor matrix X.\nY_scores: A matrix of scores (latent variables) for the response matrix Y.\nY_loadings: A matrix of loadings for the response matrix Y.\nprojection: The projection matrix used to convert X to scores.\nX_means: A vector of means of the X variables (used for centering).\nY_means: A vector of means of the Y variables (used for centering).\nfitted_values: An array of fitted values for the response matrix Y.\nresiduals: An array of residuals for the response matrix Y.\nX_variance: A vector containing the amount of variance in X explained by each   component.\nX_total_variance: The total variance in X.\ngammas: The power parameter (γ) values obtained during power optimization.\ncanonical_correlations: Canonical correlation values for each component.\nsmall_norm_indices: Indices of explanatory variables with norms close to or equal to   zero.\ncanonical_coefficients: A matrix containing the canonical coefficients (a) from  canonical correlation analysis (cor(Za, Yb)).\nsample_labels: The provided sample labels (or an empty vector if none were supplied).\npredictor_labels: The provided predictor labels (or an empty vector).\nresponse_labels: The provided response labels (or an empty vector).\nanalysis_mode: Tracks whether the model was fit for regression or discriminant analysis.\nda_categories: The original categorical responses for discriminant analysis (otherwise nothing).\n\nNotes\n\nThe CPPLS model is an extension of Partial Least Squares (PLS) that incorporates  canonical correlation analysis (CCA) and power parameter optimization to maximize the  correlation between linear combinations of X and Y.\nThe power parameter (γ) controls the balance between variance maximization and  correlation maximization. It is optimized within the specified bounds (gamma_bounds).\nIf Y_auxiliary is provided, it is concatenated with Y to form a combined response  matrix (Y_combined), which is used during the fitting process.\nPassing a categorical response vector instead of a numeric matrix automatically triggers the discriminant-analysis variant of fit_cppls and infers class labels.\n\nExample\n\njulia> X = Float64[1 0 2\n                   0 1 2\n                   1 1 1\n                   2 3 0\n                   3 2 1];\n\njulia> labels = [\"red\", \"blue\", \"red\", \"blue\", \"red\"];\n\njulia> Y, classes = labels_to_one_hot(labels);\n\njulia> model = fit_cppls(X, Y, 2; gamma=(0.7, 1.0));\n\njulia> model.X_means ≈ Matrix([1.4 1.4 1.2])\n\njulia> model.gammas ≈ [0.700185836799654, 0.9366214237592033]\ntrue\n\n\n\n\n\nfit_cppls(X, labels::AbstractCategoricalArray, n_components=2; kwargs...)\nfit_cppls(X, labels::AbstractVector, n_components=2; kwargs...)\n\nDiscriminant-analysis variants of fit_cppls. The first method dispatches specifically on CategoricalVector/CategoricalArray inputs so users can opt into DA behaviour through the type signature alone. The second method accepts any other label container (e.g. plain Vector{String} or Vector{Symbol}) but follows the exact same code path. Both convert the labels to a one-hot response matrix internally and store the inferred class names inside the returned CPPLS model.\n\nExample\n\njulia> using CategoricalArrays\n\njulia> X = Float64[1 0; 0 1; 1 1; 2 1];\n\njulia> cat_labels = categorical([\"red\", \"blue\", \"red\", \"blue\"]);\n\njulia> cppls_cat = fit_cppls(X, cat_labels, 2; gamma=0.5);\n\njulia> cppls_cat.analysis_mode\n:discriminant\n\njulia> plain_labels = [\"red\", \"blue\", \"red\", \"blue\"];\n\njulia> cppls_plain = fit_cppls(X, plain_labels, 2; gamma=0.5);\n\njulia> cppls_plain.response_labels == cppls_cat.response_labels\ntrue\n\n\n\n\n\nfit_cppls(X::AbstractMatrix{<:Real}, y::AbstractVector{<:Real}, n_components=2; kwargs...)\n\nRegression-friendly convenience wrapper around fit_cppls that accepts a single numeric response vector instead of a full response matrix. The vector is reshaped to (n_samples, 1) internally and all keyword arguments are forwarded to the standard matrix-based implementation.\n\nExample\n\njulia> X = Float64[1 2; 3 4; 5 6];\n\njulia> y = [0.1, 0.5, 0.9];\n\njulia> model = fit_cppls(X, y, 2; gamma=0.5);\n\njulia> model.analysis_mode\n:regression\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/fit/#StatisticalProjections.fit_cppls_light","page":"Fit","title":"StatisticalProjections.fit_cppls_light","text":"fit_cppls_light(\n    X::AbstractMatrix{<:Real},\n    Y::AbstractMatrix{<:Real},\n    n_components::Integer;\n    gamma::Union{<:Real, <:NTuple{2, <:Real}, <:AbstractVector{<:Union{<:Real, <:NTuple{2, <:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real}, Nothing}=nothing,\n    Y_auxiliary::Union{LinearAlgebra.AbstractVecOrMat, Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64),\n    gamma_optimization_tolerance::Real=1e-4,\n    t_squared_norm_tolerance::Real=1e-10,\n    analysis_mode::Symbol=:regression)\n\nFit a CPPLS model but retain only the parts needed for prediction (CPPLSLight).\n\nArguments mirror fit_cppls, including support for scalar γ, (lo, hi) bounds, or vectors that mix scalars and tuples as candidate sets. The returned CPPLSLight stores only the stacked regression coefficients plus the X/Y centering means.  analysis_mode is an internal keyword that tags the resulting object as either a regression or discriminant model; most users rely on the wrappers below instead of setting it manually.\n\nNotes\n\nUse this when you only need predictions, not the intermediate diagnostics.\nThe same preprocessing, weighting, and tolerance settings apply as in fit_cppls.\n\nExample\n\njulia> X = Float64[1 0 2\n                   0 1 2\n                   1 1 1\n                   2 3 0\n                   3 2 1];\n\njulia> labels = [\"red\", \"blue\", \"red\", \"blue\", \"red\"];\n\njulia> Y, classes = labels_to_one_hot(labels);\n\njulia> model = fit_cppls_light(X, Y, 2; gamma=(0.7, 1.0));\n\njulia> model.X_means ≈ Matrix([1.4 1.4 1.2])\ntrue\n\n\n\n\n\nfit_cppls_light(X, labels::AbstractCategoricalArray, n_components=2; kwargs...)\nfit_cppls_light(X, labels::AbstractVector, n_components=2; kwargs...)\n\nDiscriminant-analysis convenience wrappers for fit_cppls_light. The first signature dispatches explicitly on categorical arrays so callers can rely on the method table to distinguish regression from DA. The second accepts any other label container (e.g. vectors of strings, symbols, or enums) and forwards into the same code path. Regardless of signature, labels are converted to a one-hot matrix for fitting, class names are inferred once, and the returned CPPLSLight only retains the components needed for prediction.\n\nExample\n\njulia> using CategoricalArrays\n\njulia> X = Float64[1 0; 0 1; 1 1; 2 1];\n\njulia> labels = categorical([\"classA\", \"classB\", \"classA\", \"classB\"]);\n\njulia> light_cat = fit_cppls_light(X, labels, 2; gamma=0.5);\n\njulia> light_cat.analysis_mode\n:discriminant\n\njulia> light_plain = fit_cppls_light(X, [\"classA\", \"classB\", \"classA\", \"classB\"], 2; gamma=0.5);\n\njulia> light_plain.regression_coefficients ≈ light_cat.regression_coefficients\ntrue\n\n\n\n\n\nfit_cppls_light(X::AbstractMatrix{<:Real}, y::AbstractVector{<:Real}, n_components=2; kwargs...)\n\nRegression convenience wrapper for fit_cppls_light that accepts a single numeric response vector. Internally reshapes y to (n_samples, 1) and forwards all keyword arguments to the matrix-based implementation.\n\nExample\n\njulia> X = Float64[1 2; 3 4; 5 6];\n\njulia> y = [0.1, 0.5, 0.9];\n\njulia> light = fit_cppls_light(X, y, 2; gamma=0.5);\n\njulia> light.analysis_mode\n:regression\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#Predict","page":"Predict","title":"Predict","text":"","category":"section"},{"location":"CPPLS/predict/#StatisticalProjections.predict","page":"Predict","title":"StatisticalProjections.predict","text":"predict(cppls::CPPLS, X::AbstractMatrix{<:Real},\n    n_components::Integer=size(cppls.regression_coefficients, 3)) -> Array{Float64, 3}\n\nGenerate predictions from a fitted CPPLS model for a given input matrix X.\n\nArguments\n\ncppls: A fitted CPPLS model, containing regression coefficients and mean values of  predictors and responses.\nX: A matrix of predictor variables of size (n_samples_X, n_features).\nn_components (optional): Number of CPPLS components to use for prediction. Defaults to  the full number trained in the model. Must not exceed the number available.\n\nReturns\n\nA 3-dimensional array of shape (n_samples_X, n_targets_Y, n_components):\nn_samples_X: Number of input samples (rows of X)\nn_targets_Y: Number of target variables in the CPPLS model\nn_components: Number of components used for prediction Each [:,:,i] slice corresponds to predictions using the first i components.\n\nExample\n\njulia> coeffs = reshape(Float64[0.5, 1.0], 2, 1, 1);  # two predictors, one target\n\njulia> X_mean = zeros(1, 2); Y_mean = reshape([0.0], 1, 1);\n\njulia> model = CPPLSLight(coeffs, X_mean, Y_mean, :regression);\n\njulia> Xnew = [1.0 2.0; 3.0 4.0];\n\njulia> predict(model, Xnew) ≈ [2.5; 5.5]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#StatisticalProjections.predictonehot","page":"Predict","title":"StatisticalProjections.predictonehot","text":"predictonehot(cppls::AbstractCPPLS, predictions::AbstractArray{<:Real, 3}) -> Matrix{Int}\n\nConvert a 3D array of predictions from a CPPLS model into a one-hot encoded 2D matrix,  assigning each sample to the class with the highest summed prediction across components,  after adjusting for overcounted means.\n\nArguments\n\ncppls: A fitted CPPLS model object containing the mean response vector (Y_means).\npredictions: A 3D array of predictions with dimensions (n_samples_X, n_targets_Y,  n_components).  Typically, this represents predicted values for multiple samples, targets, and components.\n\nReturns\n\nA 2D matrix of size (n_samples_X, n_targets_Y) where each row is a one-hot encoded  vector indicating the target class with the highest summed prediction across components.\n\nDetails\n\nSums predictions across all components for each sample and class.\nAdjusts the summed predictions by subtracting (n_components - 1) times the mean  response, to correct for repeated addition of the mean in each component.\nFor each sample, finds the class index with the highest adjusted prediction.\nConverts the predicted class indices to a one-hot encoded matrix.\n\nExample\n\njulia> coeffs = reshape(Float64[1, -1, 0.5, -0.5], 2, 2, 1);  # two predictors, two classes\n\njulia> X_mean = zeros(1, 2); Y_mean = reshape([0.0 0.0], 1, 2);\n\njulia> model = CPPLSLight(coeffs, X_mean, Y_mean, :regression);\n\njulia> Xnew = [2.0 1.0; 0.5 3.0];\n\njulia> raw = predict(model, Xnew);  # size 2×2×1\n\njulia> raw ≈ [1.0 0.5; -2.5 -1.25]\ntrue\n\njulia> predictonehot(model, raw) ≈ [1 0; 0 1]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#StatisticalProjections.project","page":"Predict","title":"StatisticalProjections.project","text":"project(cppls::AbstractCPPLS, X::AbstractMatrix{<:Real}) -> AbstractMatrix\n\nCompute latent component scores by projecting new predictors X with a fitted CPPLS model.\n\nArguments\n\ncppls: Any CPPLS model (e.g., CPPLS or CPPLSLight) providing X_means and projection.\nX: Predictor matrix shaped like the training data (n_samples × n_features).\n\nReturns\n\nMatrix of size (n_samples, n_components) containing the component scores.\n\nDetails\n\nCenters X by subtracting cppls.X_means, then multiplies by the projection matrix.\n\nExample\n\njulia> struct DemoCPPLS <: StatisticalProjections.AbstractCPPLS\n           projection::Matrix{Float64}\n           X_means::Matrix{Float64}\n       end\n\njulia> proj = reshape([1.0, 0.5], 2, 1)\n2×1 Matrix{Float64}:\n 1.0\n 0.5\n\njulia> demo = DemoCPPLS(proj, reshape([0.5, 0.5], 1, :));\n\njulia> project(demo, [1.0 2.0; 3.0 4.0]) ≈ [1.25; 4.25]\ntrue\n\nIn practice, demo would be the CPPLS object returned by fit_cppls, which already contains the appropriate projection matrix and predictor means.\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/predict/#StatisticalProjections.decision_line","page":"Predict","title":"StatisticalProjections.decision_line","text":"decision_line(cppls::CPPLS; dims=(1, 2), n_components=maximum(dims))\n\nReturn the discriminant hyperplane restricted to the selected score dimensions as a tuple (xs, ys, intercept, normal). Use xs/ys to draw the line directly on a score plot; intercept and normal describe the underlying equation normal⋅scores + intercept = 0.\n\nKeywords\n\ndims: Two component indices that define the score plane. Defaults to the first two components shown in typical score plots.\nn_components: Number of latent components used in the classifier. By default matches the largest index in dims, ensuring the decision line reflects the same submodel whose scores are plotted.\n\nNotes\n\nWorks for discriminant CPPLS fits with exactly two response classes. The prediction step is linear in the latent scores, so fitting normal⋅scores + intercept = (Ŷ₁ - Ŷ₂) via least squares recovers the separating line in the requested score plane.\nThe returned (xs, ys) span the range of the training scores along dims. Offset a small margin so the boundary extends slightly beyond the cloud of points.\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/crossvalidation/#Cross-Validation","page":"Cross Validation","title":"Cross Validation","text":"","category":"section"},{"location":"CPPLS/crossvalidation/#StatisticalProjections.calculate_p_value","page":"Cross Validation","title":"StatisticalProjections.calculate_p_value","text":"calculate_p_value(permutation_accuracies::AbstractVector{<:Real},\n                  model_accuracy::Float64)\n\nCompute an empirical p-value from permutation test accuracies. Counts how many permutation  accuracies are less than or numerically equal to the observed model_accuracy, divides by  length(permutation_accuracies) + 1 to include the observed model in the denominator.\n\nArguments\n\npermutation_accuracies: vector of accuracies from label-shuffled runs.\nmodel_accuracy: accuracy achieved by the true model.\n\nExample\n\njulia> calculate_p_value([0.4, 0.5, 0.55, 0.6], 0.58) ≈ 0.6\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/crossvalidation/#StatisticalProjections.nested_cv","page":"Cross Validation","title":"StatisticalProjections.nested_cv","text":"nested_cv(X_predictors::AbstractMatrix{<:Real}, Y_responses::AbstractMatrix{<:Integer};\n    gamma::Union{<:Real, <:NTuple{2, <:Real}, <:AbstractVector{<:Union{<:Real,<:NTuple{2, <:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real},Nothing}=nothing,\n    Y_auxiliary::Union{AbstractMatrix{<:Real},Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64),\n    t_squared_norm_tolerance::Real=1e-10,\n    gamma_optimization_tolerance::Real=1e-4,\n    num_outer_folds::Integer=8,\n    num_outer_folds_repeats::Integer=num_outer_folds,\n    num_inner_folds::Integer=7,\n    num_inner_folds_repeats::Integer=num_inner_folds,\n    max_components::Integer=5,\n    weighted_nmc::Bool=true,\n    rng::AbstractRNG=Random.GLOBAL_RNG,\n    verbose::Bool=true)\n\nTop-level nested CV driver for CPPLS. Parameter overview:\n\nX_predictors, Y_responses: feature and one-hot response matrices (integer-valued). Wrapper methods accept categorical label vectors. Regression responses (real-valued vectors or matrices) are currently unsupported because only the NMC metric is implemented.\ngamma: either a scalar γ, a (lo, hi) tuple, or a vector of mixed candidates passed to fit_cppls_light. Scalars enforce a single γ for all components, whereas tuples/vectors share candidate ranges from which each component selects its own optimum.\nobservation_weights: optional sample weights; Y_auxiliary: extra response features.\ncenter: toggle mean-centering; tolerances control numerical stability inside inner fits; weighted_nmc chooses between weighted/unweighted misclassification cost.\nnum_outer_folds, num_outer_folds_repeats: number of outer stratified folds and how many to evaluate (≤ num_outer_folds).\nnum_inner_folds, num_inner_folds_repeats: same for inner CV.\nmax_components: maximum latent components considered by the inner loop.\nrng: random generator shared across fold shuffles; verbose: emit progress.\n\nFor every outer fold the data is split into training/test partitions, an inner CV loop selects the optimal component count via optimize_num_latent_variables, the final CPPLSLight is fit on the outer training data with that count, and accuracy = 1 - nmc is computed on the outer test split. Returns a tuple (outer_fold_accuracies, optimal_num_latent_variables).\n\nExample\n\njulia> using Random\n\njulia> X = rand(MersenneTwister(1), 12, 4);\n\njulia> labels = repeat([\"red\", \"blue\", \"green\"], 4);\n\njulia> Y, _ = labels_to_one_hot(labels);\n\njulia> accs, comps = nested_cv(\n           X, Y;\n           gamma=0.5,\n           num_outer_folds=3,\n           num_inner_folds=2,\n           max_components=2,\n           rng=MersenneTwister(2),\n           verbose=false);\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\njulia> accs ≈ [1.1102230246251565e-16, 0.0, 0.0]\ntrue\n\njulia> comps ≈ [1, 1, 1]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/crossvalidation/#StatisticalProjections.nested_cv_permutation","page":"Cross Validation","title":"StatisticalProjections.nested_cv_permutation","text":"nested_cv_permutation(X_predictors::AbstractMatrix{<:Real}, Y_responses::AbstractMatrix{<:Integer};\n    gamma::Union{<:Real, <:NTuple{2,<:Real}, <:AbstractVector{<:Union{<:Real,<:NTuple{2,<:Real}}}}=0.5,\n    observation_weights::Union{AbstractVector{<:Real},Nothing}=nothing,\n    Y_auxiliary::Union{AbstractMatrix{<:Real},Nothing}=nothing,\n    center::Bool=true,\n    X_tolerance::Real=1e-12,\n    X_loading_weight_tolerance::Real=eps(Float64),\n    t_squared_norm_tolerance::Real=1e-10,\n    gamma_optimization_tolerance::Real=1e-4,\n    num_outer_folds::Integer=9,\n    num_outer_folds_repeats::Integer=num_outer_folds,\n    num_inner_folds::Integer=8,\n    num_inner_folds_repeats::Integer=num_inner_folds,\n    max_components::Integer=5,\n    weighted_nmc::Bool=true,\n    num_permutations::Integer=999,\n    rng::AbstractRNG=Random.GLOBAL_RNG,\n    verbose::Bool=true)\n\nPermutation-based significance test for the nested CV pipeline. Keywords mirror nested_cv but with explicit defaults suited for permutation tests. Parameter summary:\n\ngamma, observation_weights, Y_auxiliary, center, tolerances, weighted_nmc: forwarded directly into each nested CV call. Only discriminant responses (integer one-hot matrices or categorical vectors) are supported because the scoring metric is classification-specific.\nnum_outer_folds, num_outer_folds_repeats, num_inner_folds, num_inner_folds_repeats, max_components: control the inner/outer fold geometry per permutation.\nnum_permutations: number of label shuffles (≥ 1).\nrng: governs shuffling of both labels and folds; verbose: prints progress.\n\nFor each permutation, the rows of Y_responses are randomly shuffled, then nested_cv is executed with the same hyperparameters, and the mean outer-fold accuracy is recorded. Returns a vector of length num_permutations containing those mean accuracies so you can compute empirical p-values against the unpermuted nested-CV accuracy.\n\nExample\n\njulia> using Random\n\njulia> X = rand(MersenneTwister(1), 12, 4);\n\njulia> labels = repeat([\"red\", \"blue\", \"green\"], 4);\n\njulia> Y, classes = labels_to_one_hot(labels);\n\njulia> perms = nested_cv_permutation(X, Y;\n                 gamma=0.5,\n                 num_outer_folds=3,\n                 num_inner_folds=2,\n                 max_components=2,\n                 num_permutations=2,\n                 verbose=false,\n                 rng=MersenneTwister(2));\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 2 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 3 (size = 3) not evenly divisible by 2 batches.\n[ Info: Stratum 1 (size = 3) not evenly divisible by 2 batches.\n\njulia> perms ≈ [0.3055555555555556, 0.22222222222222224]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/internal/#Internal","page":"Internal","title":"Internal","text":"","category":"section"},{"location":"CPPLS/internal/#StatisticalProjections.nmc","page":"Internal","title":"StatisticalProjections.nmc","text":"StatisticalProjections.nmc(Y_true_one_hot::AbstractMatrix{<:Integer}, \n    Y_pred_one_hot::AbstractMatrix{<:Integer}, weighted::Bool)\n\nCompute the normalized misclassification cost between true and predicted one-hot label  matrices. If weighted is false, the function returns the plain misclassification rate  (mean of entry-wise inequality). When true, class weights inversely proportional to  their prevalence are applied, so rare classes contribute equally.\n\nArguments\n\nY_true_one_hot: (n_samples × n_classes) ground truth one-hot labels.\nY_pred_one_hot: predicted one-hot labels of the same shape.\nweighted: toggle class-balanced weighting.\n\nReturns a Float64 between 0 and 1.\n\nExample\n\njulia> Y_true = [1 0 0; 0 1 0; 0 1 0];\n\njulia> Y_pred = [1 0 0; 0 0 1; 0 1 0];\n\njulia> StatisticalProjections.nmc(Y_true, Y_pred, false) ≈ 0.2222222222222222\ntrue\n\njulia> StatisticalProjections.nmc(Y_true, Y_pred, true) ≈ 0.25\ntrue\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/internal/#StatisticalProjections.optimize_num_latent_variables","page":"Internal","title":"StatisticalProjections.optimize_num_latent_variables","text":"StatisticalProjections.optimize_num_latent_variables(\n    X_train_full::AbstractMatrix{<:Real},\n    Y_train_full::AbstractMatrix{<:Integer},\n    max_components::Integer,\n    num_inner_folds::Integer,\n    num_inner_folds_repeats::Integer,\n    gamma::Union{<:Real, <:NTuple{2,<:Real}, <:AbstractVector{<:Union{<:Real, <:NTuple{2, <:Real}}}},\n    observation_weights::Union{AbstractVector{<:Real},Nothing},\n    Y_auxiliary::Union{AbstractMatrix{<:Real},Nothing},\n    center::Bool,\n    X_tolerance::Real,\n    X_loading_weight_tolerance::Real,\n    t_squared_norm_tolerance::Real,\n    gamma_optimization_tolerance::Real,\n    weighted_nmc::Bool,\n    rng::AbstractRNG,\n    verbose::Bool)\n\nRepeated inner cross-validation used inside nested_cv to pick the component count. Argument summary:\n\nX_train_full, Y_train_full: numeric training matrices (observations × features/targets). Y_train_full must be an integer one-hot matrix for discriminant analysis. Wrapper methods accept categorical label vectors and convert them automatically.\nmax_components: Int upper bound on components to evaluate (≥ 1).\nnum_inner_folds, num_inner_folds_repeats: integers controlling stratified folds drawn via random_batch_indices.\ngamma: either a scalar γ, a (lo, hi) tuple of Reals, or a vector mixing both; forwarded to fit_cppls_light. Scalars keep γ fixed for every component, while tuples/vectors let each component pick the best γ from the shared candidate set.\nobservation_weights: optional weight vector matching the training rows.\nY_auxiliary: optional auxiliary response matrix aligned with Y_train_full.\ncenter: Bool toggling mean-centering in the inner fits.\nX_tolerance, X_loading_weight_tolerance, t_squared_norm_tolerance, gamma_optimization_tolerance: Real tolerances passed to the fitter.\nweighted_nmc: choose class-weighted misclassification cost (true by default).\nrng: random-number generator used for shuffling.\nverbose: when true, prints per-fold diagnostics.\n\nExample\n\njulia> using Random\n\njulia> X = rand(MersenneTwister(1), 12, 4);\n\njulia> labels = repeat([\"red\", \"blue\", \"green\"], 4);\n\njulia> Y, _ = labels_to_one_hot(labels);\n\njulia> k = StatisticalProjections.optimize_num_latent_variables(\n             X, Y,\n             2,\n             3, 3,\n             0.5,\n             nothing, nothing,\n             true,\n             1e-12, eps(Float64), 1e-10,\n             1e-4,\n             true,\n             MersenneTwister(2),\n             false);\n[ Info: Stratum 2 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 3 (size = 4) not evenly divisible by 3 batches.\n[ Info: Stratum 1 (size = 4) not evenly divisible by 3 batches.\njulia> k\n1\n\nFor every inner repeat the routine fits a CPPLSLight with max_components, scores validation folds for each partial component count using predict + predictonehot, evaluates nmc, records the argmin, and finally returns the median winning component number (rounded down) across repeats.\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/internal/#StatisticalProjections.random_batch_indices","page":"Internal","title":"StatisticalProjections.random_batch_indices","text":"StatisticalProjections.random_batch_indices(strata::AbstractVector{<:Integer},\n    num_batches::Integer, rng::AbstractRNG=Random.GLOBAL_RNG)\n\nConstruct stratified folds. For each unique entry in strata the corresponding sample indices are shuffled with rng and then dealt round-robin into num_batches disjoint vectors. This keeps class proportions stable across folds. Throws if num_batches is less than 1 or larger than the number of samples. Returns a vector-of-vectors of 1-based indices, each representing one fold.\n\nExample\n\njulia> using Random; rng = MersenneTwister(1);\n\njulia> folds = StatisticalProjections.random_batch_indices([1, 1, 2, 2, 2, 1], 3, rng)\n3-element Vector{Vector{Int64}}:\n [5, 6]\n [4, 1]\n [3, 2]\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/theory/#Canonical-Powered-Partial-Least-Squares-(CPPLS)","page":"Canonical Powered Partial Least Squares (CPPLS)","title":"Canonical Powered Partial Least Squares (CPPLS)","text":"Canonical Powered Partial Least Squares (CPPLS) is a supervised projection method for regression and classification. Its purpose is to extract latent components that summarize the predictor matrix X in mathbbR^n times p in a way that best reflects the structure in a multivariate response matrix Y in mathbbR^n times q. The method extends standard PLS in three important ways. First, it allows multiple response variables, including both primary responses that one wishes to predict and optional auxiliary responses that guide the extraction of components. Second, it incorporates a power parameter gamma that controls the balance between predictor variance and predictor–response correlation, giving the user explicit control over how strongly the model should emphasize correlation structure. Third, CPPLS can operate with a vector of non-negative sample weights, allowing some observations to contribute more or less to the fitted model. This is useful when classes are unbalanced, when some samples are more reliable, or when experimental design considerations suggest that certain samples should carry greater influence.\n\nEach CPPLS component is extracted in two conceptual stages. First, the predictors are projected onto supervised directions, one for each column of Y, using the gamma-controlled mixture of weighted predictor variance and weighted predictor–response correlation. Second, a canonical correlation analysis (CCA) determines how these supervised directions should be linearly combined into a single latent variable that is optimally aligned with the primary responses. Auxiliary responses and observation weights enter the computation of supervised directions in the first stage, shaping the latent space that is subsequently analyzed by CCA, while the CCA itself is guided solely by the primary responses under the same weighting structure.\n\nTo begin, X and Y are centered using the supplied sample weights. If w_i is the weight of sample i and the weights are normalized to sum to one, the weighted mean of a variable x becomes\n\nbarx = sum_i w_i x_i \n\nAll variances, covariances, and correlations are computed in a weighted sense. For a centered variable x, the weighted variance is\n\noperatornameVar_w(x) = sum_i w_i x_i^2 \n\nand for two centered variables x and y, the weighted covariance is\n\noperatornameCov_w(xy) = sum_i w_i x_i y_i \n\nWeighted correlations are obtained by normalizing the weighted covariance by the corresponding weighted standard deviations.\n\nCPPLS constructs a supervised transformation matrix by combining predictor scale and predictor–response correlation, with the balance controlled by the power parameter gamma in (01). For each predictor x_j (a column of X) and each response y_k (a column of Y), CPPLS computes the weighted standard deviation operatornamestd_w(x_j) and the weighted correlation operatornamecorr_w(x_jy_k). These quantities are not combined additively, but multiplicatively through gamma-dependent powers.\n\nThe resulting supervised weight matrix is\n\nW_0(gamma) in mathbbR^p times q\n\nwhere p is the number of predictors and q is the number of response columns, including auxiliary ones. It can be written as a product of a diagonal scale matrix and a correlation matrix,\n\nW_0(gamma) = S_x(gamma)C(gamma)\n\nwith diagonal entries\n\nS_x(gamma)_jj = operatornamestd_w(x_j)^frac1-gammagamma\n\nand correlation entries\n\nC(gamma)_jk = operatornamesignbig(operatornamecorr_w(x_jy_k)big) leftoperatornamecorr_w(x_jy_k)right^fracgamma1-gamma \n\nThus, each entry of W_0(gamma) is proportional to a product of a predictor-scale term and a predictor–response correlation term, each raised to a power determined by gamma. When gamma is small, predictors with large weighted standard deviation are emphasized; when gamma approaches one, predictors that are strongly correlated with the responses dominate. This power-based variance–correlation trade-off replaces the need for external scaling of the data.\n\nEach column of W_0(gamma) is a supervised direction in the original predictor space. If auxiliary responses are supplied, they contribute additional columns and thereby enrich the set of supervised directions. Because the construction of W_0(gamma) uses the sample weights, heavily weighted samples exert proportionally greater influence on the supervised compression.\n\nMultiplying the predictor matrix by this weight matrix gives the gamma-dependent supervised compression\n\nZ(gamma) = X W_0(gamma)\n\nwhich has the same number of samples as X but one column for each response variable. Every entry in Z(gamma) is a single summary value for a sample in the supervised direction associated with a response column. Auxiliary responses provide additional supervised views of the data, and sample weights ensure that classes or samples with higher weights have greater influence on how these supervised directions are shaped.\n\nTo determine the optimal balance between variance and correlation, CPPLS evaluates a grid of gamma-values. For each candidate gamma, CPPLS computes Z(gamma) and performs a weighted canonical correlation analysis between Z(gamma) and the primary responses. Only the first canonical correlation is kept and serves as a score indicating how well that gamma captures structure relevant to the primary response. The optimal value gamma_mathrmbest is the one maximizing this weighted canonical correlation. This step does not yet produce components or deflate the data; it merely evaluates each candidate gamma under identical conditions.\n\nOnce the optimal gamma has been determined, CPPLS recomputes\n\nZ = Z(gamma_mathrmbest)\n\nand performs a full weighted CCA between this matrix and the primary response columns of Y. The result is a canonical direction a in the Z-space and a corresponding direction b in the primary response space. The direction a specifies how to combine the supervised directions in Z into one axis that maximally correlates with the primary responses. Auxiliary responses influence this direction indirectly, since they have already shaped the supervised directions in W_0(gamma_mathrmbest). The canonical direction is then mapped back into the predictor space through\n\nw = W_0(gamma_mathrmbest) a \n\nproducing the final CPPLS weight vector. This vector lies in the original predictor space and defines the direction used to compute the component score\n\nt = X w \n\nThe component score t acts as a latent one-dimensional slider: each sample receives a coordinate t_i, and moving along this latent axis corresponds to sliding along the component in predictor space. The relationship between the component and the original variables is captured by the loadings. The weighted X-loading is given by\n\np = fracX^top W tt^top W t\n\nwhich is the weighted regression of the predictors on the component. It describes how each predictor changes as one moves along t. The weighted Y-loading\n\nc = fracY^top W tt^top W t\n\ndescribes how each response variable—including auxiliary responses when present—varies with the component under the weighting structure.\n\nDeflation removes the part of X and Y that can be explained by this component:\n\nX leftarrow X - t p^topqquad\nY leftarrow Y - t c^top \n\nAfter this deflation, the dominant species structure has been removed from both X and Y. Because subsequent components are extracted from these residuals, they often describe remaining structured variation—such as seasonal drift or batch effects—rather than additional species separation. This does not impair discrimination: the first component captures the primary class separation, and later components model the remaining confounding structure, improving stability and interpretability.\n\nSample weighting becomes particularly important in discriminant analysis (CPPLS-DA) when classes are unbalanced or when certain samples are more reliable. For example, if one species has 20 samples and another 60, the unweighted analysis gives the larger class three times the influence on variances and correlations, and thus on the supervised compression. This often causes the dominant component to reflect within-class variation of the majority class rather than the between-class difference one intends to model. By assigning larger weights to minority-class samples and smaller weights to majority-class samples, the total effective weight of each class becomes balanced. All weighted variances, covariances, and correlations then reflect this adjusted influence, and the extracted components focus on genuine between-class separation rather than being overwhelmed by the larger class. Similarly, samples known to be noisy or unreliable can be down-weighted, while representative or carefully controlled samples can be up-weighted, improving the robustness of the extracted components.\n\nA concrete example illustrates the benefit of combining auxiliary responses and sample weighting. Suppose two insect species are analyzed by GC–MS to characterize their cuticular hydrocarbons. The primary task is to discriminate species, but chemical profiles change with season, and species may not be collected uniformly throughout the year. In this situation, the largest variation in X may reflect seasonal drift rather than species. Even if the response Y encodes only species, the supervised compression may inadvertently emphasize seasonal structure, because GC–MS peaks that vary with season often correlate indirectly with species when sampling times differ. Including sampling date or season as an auxiliary Y-column provides CPPLS with a supervised direction dedicated to seasonal variation, preventing this structure from leaking into the species component. If sampling imbalance is also present—say one species is collected mostly early in the season and the other mostly later—assigning balanced sample weights prevents the majority class or sampling pattern from dominating the supervised compression. Together, auxiliary Y and sample weighting produce a far more stable and interpretable latent structure: the first component becomes species-focused, while secondary components capture the residual seasonal drift or other confounders. X-loadings highlight species-specific peaks, while seasonal peaks are isolated to components guided by the auxiliary response.\n\nAfter all components are extracted, regression coefficients for predicting the primary responses are assembled using\n\nB =\nW_mathrmcomp\nleft( P^top W_mathrmcomp right)^-1\nC_mathrmprimary^top \n\nwhere W_mathrmcomp contains the component weight vectors, P the corresponding X-loadings, and C_mathrmprimary the primary Y-loadings. Auxiliary responses influence the latent components but do not appear in the final regression model unless explicitly designated as prediction targets.\n\nIn summary, CPPLS combines three complementary forms of supervision: the power parameter gamma that controls the balance between variance and correlation, auxiliary responses that provide additional structured guidance for the supervised compression, and sample weights that ensure appropriate influence of different samples or classes. Together, these features allow CPPLS to build stable, interpretable, and discriminative models even in complex, high-dimensional, and confounded data settings.","category":"section"},{"location":"Utils/internal/#Internal","page":"Internal","title":"Internal","text":"","category":"section"},{"location":"Utils/internal/#StatisticalProjections.robustcor","page":"Internal","title":"StatisticalProjections.robustcor","text":"StatisticalProjections.robustcor(x::AbstractVector, y::AbstractVector)\n\nRobust correlation helper used inside projection diagnostics. Returns the Pearson  correlation between x and y, falling back to 0.0 when either input is constant or  when the computed value is not finite (e.g. NaN or Inf).\n\nExamples\n\njulia> StatisticalProjections.robustcor([1, 2, 3], [3, 2, 1])\n-1.0\n\njulia> StatisticalProjections.robustcor([1, 1, 1], [2, 3, 4])\n0.0\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"CPPLS/visualization/#StatisticalProjections.scoreplot","page":"Visualization","title":"StatisticalProjections.scoreplot","text":"scoreplot(cppls; kwargs...) -> Makie.FigureAxisPlot / Plot\n\nKeyword-friendly wrapper around the Makie recipe for CPPLS score plots. Accepts any CPPLS model (or arguments compatible with scoreplotplot) and forwards keywords down to the recipe while supplying sensible axis defaults (\"Compound 1\" and \"Compound 2\" unless you override them with the axis keyword).\n\nColor handling is specialised. When the fitted model stores discriminant-analysis labels (cppls.da_categories), samples are colored by group automatically. You can override this via color:\n\nScalar color ⇒ every sample uses that color.\nVector/Tuple ⇒ treated as a palette. Because the plot colors discriminant groups automatically, the length must match the number of unique labels (order follows the stored response labels). If color matches the number of rows it is interpreted per sample.\n\nScatter-style attributes such as marker, markersize, strokecolor, strokewidth, alpha, and the scoreplot-specific dims keyword (selecting which CPPLS score components to display) pass straight through to Makie.\n\nReturns the Plot object created by scoreplotplot, matching Makie’s usual figure/axis semantics. The implementation itself is provided by the Makie extension module once Makie is loaded.\n\n\n\n\n\n","category":"function"},{"location":"CPPLS/visualization/#StatisticalProjections.scoreplot!","page":"Visualization","title":"StatisticalProjections.scoreplot!","text":"scoreplot!(axis::Makie.AbstractAxis, cppls; kwargs...) -> Plot\nscoreplot!(args...; kwargs...) -> Plot\n\nIn-place variants of scoreplot that draw into an existing Makie axis (first form) or accept the same positional arguments Makie’s scoreplotplot! expects (second form). Both forward the scatter keywords (color, marker, markersize, strokecolor, strokewidth, alpha, …) plus the scoreplot-specific dims selector and keep automatic axis labelling unless you override xlabel/ylabel. Returns the created Plot.\n\n\n\n\n\n","category":"function"},{"location":"Utils/statistics/#Statistics","page":"Statistics","title":"Statistics","text":"","category":"section"},{"location":"Utils/statistics/#StatisticalProjections.fisherztrack","page":"Statistics","title":"StatisticalProjections.fisherztrack","text":"fisherztrack(X::AbstractArray{<:Real, 3}, scores::AbstractVector; weights=:mean)\n\nInterpret X as a three-dimensional array (a “tensor”) of shape n × axis₁ × axis₂, where n matches the length of scores. For every combination of axis₁ and axis₂,  the function extracts the length-n slice (a single “track”) and correlates it with  scores. Those correlations are Fisher z-transformed to stabilize variance, optionally  weighted by the slice means (when weights = :mean), averaged for each axis₁, and  finally inverse-transformed. The result is one smoothed correlation per axis₁,  summarizing all its axis₂ slices.\n\nArguments\n\nX: 3-D tensor whose first axis matches the observation axis of scores.\nscores: response to correlate with every slice in X.\nweights: choose :mean to weight by the slice means or :none for equal weights.\n\nReturns a vector of correlations with length size(X, 2).\n\nExample\n\njulia> X = reshape(Float64[1, 2, 3, 4, 2, 3, 4, 5, 3, 5, 7, 9], 4, 3, 1);\n\njulia> scores = [1.0, 2.0, 3.0, 4.0];\n\njulia> fisherztrack(X, scores) ≈ [1.0, 1.0, 1.0]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"Utils/statistics/#StatisticalProjections.separationaxis","page":"Statistics","title":"StatisticalProjections.separationaxis","text":"separationaxis(Xscores::AbstractMatrix, Y::AbstractMatrix; method::Symbol=:centroid, \npositive_class::Integer=1)\n\nGiven Xscores (rows = samples, columns = latent components) and a binary one-hot label  matrix Y with two columns, this helper finds the line in score space that best separates  the classes. It returns (direction, scores) where direction is a unit vector and  scores = Xscores * direction are the signed projections, flipped if necessary so that the  positive_class has larger values.\n\nWhen Xscores has multiple columns, choose method = :centroid to use the difference of  class means or method = :lda to use Fisher’s linear discriminant (pooled covariance). If  Xscores has only one column, the function just selects the orientation that makes the  positive_class larger on average.\n\nExamples\n\njulia> X = [1.0 0.0; 2.0 1.0; 0.0 1.0; 1.0 2.0];\n\njulia> Y = [1 0; 1 0; 0 1; 0 1];\n\njulia> direction, scores = separationaxis(X, Y; method=:centroid);\njulia> direction ≈ [0.7071067811865474, -0.7071067811865474]\ntrue\n\njulia> scores ≈ [0.7071067811865474, 0.7071067811865474, -0.7071067811865474, -0.7071067811865474]\ntrue\n\n\n\n\n\n","category":"function"},{"location":"#StatisticalProjections.jl","page":"Home","title":"StatisticalProjections.jl","text":"StatisticalProjections provides chemometric projection methods in Julia, currently centered on CPPLS-DA for supervised classification tasks. The goal is to enable reproducible preprocessing, fitting, and validation pipelines, so common chemometric analyses stay transparent and auditable.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"The package is not registered, so install it directly from GitHub:\n\njulia> ]\npkg> add https://github.com/oniehuis/StatisticalProjections.jl\n\nAfter the installation finishes you can load it in the Julia REPL with:\n\njulia> using StatisticalProjections","category":"section"},{"location":"#Current-capabilities","page":"Home","title":"Current capabilities","text":"A pure-Julia implementation of Canonical Powered Partial Least Squares Discriminant Analysis (CPPLS-DA; Indahl et al. 2019, Liland & Indahl 2009) that handles collinear  predictors and exports interpretable loadings and scores.\nCross-validation utilities (nested_cv, nested_cv_permutation) for selecting the number of latent components and estimating classification performance or permutation baselines (Smit et al. 2007; currently only validated for discriminant/classification models).\nPermutation-based significance testing via calculate_p_value to quantify whether observed accuracies exceed what would be expected by chance.\nOptional preprocessing utilities so that scaling, centering, or other chemometric transformations can be folded into the modeling workflow.","category":"section"},{"location":"#Quick-taste","page":"Home","title":"Quick taste","text":"using StatisticalProjections\nusing Random\nusing Statistics\n\nrng = MersenneTwister(1)\nX = randn(rng, 60, 30)                                     # predictors (e.g., spectra)\nlabels = repeat([\"classA\", \"classB\"], inner=30)\nY, _ = labels_to_one_hot(labels)\n\naccuracies, components = nested_cv(\n    X, Y;\n    max_components=2,\n    num_outer_folds=3,\n    num_inner_folds=2,\n    rng=rng,\n    verbose=false)\n\nbest_components = floor(Int, median(components))           # consensus components across folds\nmodel = fit_cppls_light(X, Y, best_components; gamma=0.5)\nŷ = predictonehot(model, predict(model, X))                # fitted class indicators\n\npermutation_scores = nested_cv_permutation(\n    X, Y;\n    max_components=2,\n    num_outer_folds=3,\n    num_inner_folds=2,\n    num_permutations=25,\n    rng=rng,\n    verbose=false)\n\np_value = calculate_p_value(permutation_scores, mean(accuracies))\n\nThe calculated p_value reports the empirical probability of obtaining mean accuracies this high when class labels are randomly permuted, so smaller values suggest structure unlikely to arise from chance alone.","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"Learn how to fit models (options, preprocessing, γ-search) in CPPLS/fit.md and how to generate projections or class predictions in CPPLS/predict.md.\nDive into the cross-validation and permutation tooling described in CPPLS/crossvalidation.md.\nInspect the underlying data structures (CPPLS, CPPLSLight, preprocessing helpers) once you need finer control in CPPLS/types.md and CPPLS/internal.md.","category":"section"},{"location":"#Disclaimer","page":"Home","title":"Disclaimer","text":"StatisticalProjections is research software provided “as is.” You remain responsible for validating every discriminant analysis and any downstream decision or deployment based on these models; the authors cannot be held liable if the algorithms produce misleading or incorrect results.","category":"section"},{"location":"#References","page":"Home","title":"References","text":"Indahl UG, Liland KH, Naes T (2009) Canonical partial least squares — a unified PLS  approach to classification and regression problems. Journal of Chemometrics 23: 495-504.  https://doi.org/10.1002/cem.1243.\nLiland KH, Indahl UG (2009): Powered partial least squares discriminant analysis.  Journal of Chemometrics 23: 7-18. https://doi.org/10.1002/cem.1186.\nSmit S, van Breemen MJ, Hoefsloot HCJ, Smilde AK, Aerts JMFG, de Koster CG (2007):  Assessing the statistical validity of proteomics based biomarkers. Analytica Chimica  Acta 592: 210-217. https://doi.org/10.1016/j.aca.2007.04.043.","category":"section"}]
}
